# Gomoku

# To Do:
1. May add the action as an input to the Q function.
2. Add internal reward. Use log uniform to initialize. https://stackoverflow.com/questions/43977717/how-do-i-generate-log-uniform-distribution-in-python.
3. Use adam or rmsprop optimizer.
4. Use more resnet blocks. https://www.kaggle.com/readilen/resnet-for-mnist-with-pytorch
5. Population based training for hyperparameters.

# Done:
1. Tanh in (0, 1) changed to relu, because a value 1 is not approachable for tanh.
2. In each batch, memories with or without reward are balanced.
3. Add negative reward to the last move leading to lose. Otherwise Q=1 is almost a good solution to Bellman equation.
4. Action is from conv layer; no fc layer used.
